{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## `Task` Do feature selection as per metods taught is session 54 on SECOM dataset. \n",
        "\n",
        "Dataset Link : https://archive.ics.uci.edu/ml/datasets/SECOM\n",
        "\n",
        "Drive Link : https://docs.google.com/spreadsheets/d/1dFCe1zgokabsiEr6BbWmMJtiMefkrChpJWLiG_0dDkk/edit?usp=share_link"
      ],
      "metadata": {
        "id": "pxOvVQMdbqH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your Code here"
      ],
      "metadata": {
        "id": "Rcenqn0Yb5HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Solution`"
      ],
      "metadata": {
        "id": "I33nmJc-cJs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "DATA = pd.read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQtBXo5cBnDsM2fmfHPm6u72KGUS5FjPHNGMxOfYjA9-CAhmnRpwkIw_rOR3sANJIToiUU__6fbBvig/pub?gid=572763137&single=true&output=csv\")\n",
        "\n",
        "DATA.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIQC_YRgcIux",
        "outputId": "9e13f74a-1e1d-42f7-f925-0b4f49570575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1567 entries, 0 to 1566\n",
            "Columns: 592 entries, Time to Pass/Fail\n",
            "dtypes: float64(590), int64(1), object(1)\n",
            "memory usage: 7.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "RTA3ifpdc2nt",
        "outputId": "19560678-bc13-4f8d-e416-0a5459f056ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  Time        0        1          2          3       4      5  \\\n",
              "0  2008-07-19 11:55:00  3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   \n",
              "1  2008-07-19 12:32:00  3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0   \n",
              "2  2008-07-19 13:17:00  2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   \n",
              "3  2008-07-19 14:43:00  2988.72  2479.90  2199.0333   909.7926  1.3204  100.0   \n",
              "4  2008-07-19 15:22:00  3032.24  2502.87  2233.3667  1326.5200  1.5334  100.0   \n",
              "\n",
              "          6       7       8  ...       581     582     583     584      585  \\\n",
              "0   97.6133  0.1242  1.5005  ...       NaN  0.5005  0.0118  0.0035   2.3630   \n",
              "1  102.3433  0.1247  1.4966  ...  208.2045  0.5019  0.0223  0.0055   4.4447   \n",
              "2   95.4878  0.1241  1.4436  ...   82.8602  0.4958  0.0157  0.0039   3.1745   \n",
              "3  104.2367  0.1217  1.4882  ...   73.8432  0.4990  0.0103  0.0025   2.0544   \n",
              "4  100.3967  0.1235  1.5031  ...       NaN  0.4800  0.4766  0.1045  99.3032   \n",
              "\n",
              "      586     587     588       589  Pass/Fail  \n",
              "0     NaN     NaN     NaN       NaN         -1  \n",
              "1  0.0096  0.0201  0.0060  208.2045         -1  \n",
              "2  0.0584  0.0484  0.0148   82.8602          1  \n",
              "3  0.0202  0.0149  0.0044   73.8432         -1  \n",
              "4  0.0202  0.0149  0.0044   73.8432         -1  \n",
              "\n",
              "[5 rows x 592 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f4b2d20-61c1-49c4-b2f0-464285dd7d30\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>581</th>\n",
              "      <th>582</th>\n",
              "      <th>583</th>\n",
              "      <th>584</th>\n",
              "      <th>585</th>\n",
              "      <th>586</th>\n",
              "      <th>587</th>\n",
              "      <th>588</th>\n",
              "      <th>589</th>\n",
              "      <th>Pass/Fail</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-07-19 11:55:00</td>\n",
              "      <td>3030.93</td>\n",
              "      <td>2564.00</td>\n",
              "      <td>2187.7333</td>\n",
              "      <td>1411.1265</td>\n",
              "      <td>1.3602</td>\n",
              "      <td>100.0</td>\n",
              "      <td>97.6133</td>\n",
              "      <td>0.1242</td>\n",
              "      <td>1.5005</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.5005</td>\n",
              "      <td>0.0118</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>2.3630</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-07-19 12:32:00</td>\n",
              "      <td>3095.78</td>\n",
              "      <td>2465.14</td>\n",
              "      <td>2230.4222</td>\n",
              "      <td>1463.6606</td>\n",
              "      <td>0.8294</td>\n",
              "      <td>100.0</td>\n",
              "      <td>102.3433</td>\n",
              "      <td>0.1247</td>\n",
              "      <td>1.4966</td>\n",
              "      <td>...</td>\n",
              "      <td>208.2045</td>\n",
              "      <td>0.5019</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>4.4447</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>208.2045</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-07-19 13:17:00</td>\n",
              "      <td>2932.61</td>\n",
              "      <td>2559.94</td>\n",
              "      <td>2186.4111</td>\n",
              "      <td>1698.0172</td>\n",
              "      <td>1.5102</td>\n",
              "      <td>100.0</td>\n",
              "      <td>95.4878</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>1.4436</td>\n",
              "      <td>...</td>\n",
              "      <td>82.8602</td>\n",
              "      <td>0.4958</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>3.1745</td>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.0484</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>82.8602</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-07-19 14:43:00</td>\n",
              "      <td>2988.72</td>\n",
              "      <td>2479.90</td>\n",
              "      <td>2199.0333</td>\n",
              "      <td>909.7926</td>\n",
              "      <td>1.3204</td>\n",
              "      <td>100.0</td>\n",
              "      <td>104.2367</td>\n",
              "      <td>0.1217</td>\n",
              "      <td>1.4882</td>\n",
              "      <td>...</td>\n",
              "      <td>73.8432</td>\n",
              "      <td>0.4990</td>\n",
              "      <td>0.0103</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>2.0544</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>73.8432</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-07-19 15:22:00</td>\n",
              "      <td>3032.24</td>\n",
              "      <td>2502.87</td>\n",
              "      <td>2233.3667</td>\n",
              "      <td>1326.5200</td>\n",
              "      <td>1.5334</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.3967</td>\n",
              "      <td>0.1235</td>\n",
              "      <td>1.5031</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.4800</td>\n",
              "      <td>0.4766</td>\n",
              "      <td>0.1045</td>\n",
              "      <td>99.3032</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>73.8432</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 592 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f4b2d20-61c1-49c4-b2f0-464285dd7d30')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f4b2d20-61c1-49c4-b2f0-464285dd7d30 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f4b2d20-61c1-49c4-b2f0-464285dd7d30');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Model Before Feature Selection\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Dropping Time column\n",
        "data = DATA.drop('Time', axis=1)\n",
        "\n",
        "# Filling NaN Values with random \n",
        "for column in data.columns:\n",
        "    # print(\"Column- \", column)\n",
        "    # Get the minimum and maximum values of the column\n",
        "    min_value = data[column].min()\n",
        "    max_value = data[column].max()\n",
        "\n",
        "    # Generate random numbers within the range\n",
        "    random_values = np.random.uniform(min_value, max_value, size=data[column].isnull().sum())\n",
        "    \n",
        "    # Create a Series with the random values\n",
        "    random_series = pd.Series(random_values, index=data[column][data[column].isnull()].index)\n",
        "\n",
        "    # Fill NaN values with the random series\n",
        "    data[column].fillna(random_series, inplace=True)\n",
        "\n",
        "    # Print\n",
        "    # print(data[column].isnull().sum())\n",
        "\n",
        "\n",
        "# data.isnull().sum()"
      ],
      "metadata": {
        "id": "WyKXuSJEIYPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target\n",
        "X = data.drop('Pass/Fail', axis=1)\n",
        "y = data['Pass/Fail']\n",
        "\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "\n",
        "# Initialize and train logistic regression model\n",
        "log_reg = LogisticRegression(max_iter=10000)  # Increase max_iter if it doesn't converge\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqumMAGMa6Ti",
        "outputId": "722981be-bc43-4b2d-f16a-7c668e2553ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1253, 590)\n",
            "(314, 590)\n",
            "Test accuracy: 0.8821656050955414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform filter-based feature selection on the \"UCI SECOM\" dataset, which has 592 columns and a target column called \"Pass/Fail,\" we can utilize the following methods:\n",
        "\n",
        "1. Duplicate Features:\n",
        "   - Identify and remove duplicate columns from the dataset. Columns with identical values provide redundant information and do not contribute to the prediction task.\n",
        "\n",
        "2. Variance Threshold Method:\n",
        "   - Calculate the variance of each feature.\n",
        "   - Remove features with low variance, as they tend to have little or no predictive power.\n",
        "   - Set a threshold value for variance and remove features below that threshold.\n",
        "\n",
        "3. Correlation:\n",
        "   - Compute the correlation matrix of the features.\n",
        "   - Identify highly correlated features and choose one from each highly correlated group.\n",
        "   - High correlation between features indicates redundancy, and removing one from each correlated group helps reduce multicollinearity.\n",
        "\n",
        "4. ANOVA (Analysis of Variance):\n",
        "   - Perform an ANOVA test between each feature and the target variable (\"Pass/Fail\").\n",
        "   - Select features with a significant impact on the target variable.\n",
        "   - Set a significance level (e.g., p-value threshold) for the test to determine the importance of each feature.\n",
        "\n",
        "5. Chi-Squared:\n",
        "   - Apply the Chi-Squared test between each feature and the target variable, considering both features as categorical.\n",
        "   - Select features with a significant association with the target variable.\n",
        "   - Set a significance level (e.g., p-value threshold) to determine the importance of each feature.\n",
        "\n",
        "Implementing these feature selection methods in Python using the \"UCI SECOM\" dataset can be done as follows:"
      ],
      "metadata": {
        "id": "HKJO04w_AvrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, f_classif\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "data_path = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vQtBXo5cBnDsM2fmfHPm6u72KGUS5FjPHNGMxOfYjA9-CAhmnRpwkIw_rOR3sANJIToiUU__6fbBvig/pub?gid=572763137&single=true&output=csv\"\n",
        "\n",
        "# Load the dataset\n",
        "DATA = pd.read_csv(data_path)  # Replace with the actual filename and path\n",
        "\n",
        "# Remove duplicate features\n",
        "# Get the subset of columns with duplicate values\n",
        "duplicated_cols = DATA.columns[DATA.T.duplicated()]\n",
        "\n",
        "# Remove the duplicated columns\n",
        "data = DATA.drop(columns=duplicated_cols)\n",
        "\n",
        "# Drop time column\n",
        "data.drop('Time', inplace=True, axis=1)\n",
        "\n",
        "\n",
        "# Numbers Of Columns after removing Duplicate columns\n",
        "print(\"Number of Columns - \", DATA.shape[1])\n",
        "print(\"Number of Columns after removing duplicate columns- \", data.shape[1])\n"
      ],
      "metadata": {
        "id": "UyHY-ZcG_4hh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b448d58-302f-4c8b-8f15-0c623a36a05a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Columns -  592\n",
            "Number of Columns after removing duplicate columns-  487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variance Threshold Method\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "sel = selector.fit(data)\n",
        "\n",
        "columns = data.columns[sel.get_support()]\n",
        "\n",
        "data_vt = sel.transform(data)\n",
        "\n",
        "data_vt = pd.DataFrame(data_vt, columns=columns)\n",
        "\n",
        "# Numbers Of Columns after Variance Threshold Method\n",
        "\n",
        "print(\"Number of Columns after Variance Threshold Method- \", data_vt.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IOFuBtXKZCJ",
        "outputId": "925eb672-0ff2-49de-d4b3-aedb2d31ff4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Columns after Variance Threshold Method-  315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation\n",
        "corr_matrix = data_vt.corr().abs()\n",
        "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
        "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.8)]\n",
        "data_corr = data_vt.drop(to_drop, axis=1)\n",
        "\n",
        "# Numbers Of Columns after Variance Threshold Method\n",
        "\n",
        "print(\"Number of Columns after Correlation- \", data_corr.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYImnZ9lKw62",
        "outputId": "6bdbdd10-a4cc-48f2-f5ab-f0f4ace8f4dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Columns after Correlation-  162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_corr.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13_gpr0jL8zX",
        "outputId": "e9833efb-b32f-4f65-86c2-b0d155ceee98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1567 entries, 0 to 1566\n",
            "Columns: 162 entries, 0 to Pass/Fail\n",
            "dtypes: float64(162)\n",
            "memory usage: 1.9 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ANOVA - Approach showed in the session can't be done here\n",
        "#  every rows of the dataset have some NaN values.\n",
        "# ANOVA can't be applied on data having NaN values.\n"
      ],
      "metadata": {
        "id": "Kh_puWB0FZyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can still perform ANOVA analysis by considering each column separately.\n",
        "\n",
        "ANOVA can be applied to each column individually, comparing the target variable which is Pass/Fail column in our case against the non-missing values in that specific column.\n",
        "\n",
        "We will drop NaN values of each column before performing the ANOVA"
      ],
      "metadata": {
        "id": "ylwvMkblOeg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Significance Value\n",
        "alpha = 0.05 \n",
        "\n",
        "# Columns haveing p_value less than alpha\n",
        "column_pvalues = []\n",
        "\n",
        "# Iterate over each column\n",
        "for column in data_corr.iloc[:, :-1].columns:\n",
        "    # Extract the non-missing values in the column\n",
        "    column_data = data_corr[column].dropna()\n",
        "    \n",
        "    # Perform ANOVA with the target variable\n",
        "    anova_result = f_oneway(column_data, data_corr['Pass/Fail'])\n",
        "    \n",
        "    # Print the ANOVA result or perform further analysis\n",
        "    print(f\"Column: {column} - ANOVA p-value: {anova_result.pvalue}\")\n",
        "\n",
        "    if anova_result.pvalue <= alpha:\n",
        "        column_pvalues.append((column, anova_result.pvalue))\n",
        "\n",
        "# Selecting best 100 Features - lower p-value better feature\n",
        "# Sort the column p-values in ascending order\n",
        "column_pvalues.sort(key=lambda x: x[1])\n",
        "\n",
        "# Select the top 100 columns with the lowest p-values\n",
        "selected_columns = [column for column, _ in column_pvalues[:100]]\n",
        "\n",
        "data_anova = data_corr[selected_columns+['Pass/Fail']]\n",
        "\n",
        "print(\"Number of Columns after Correlation- \", data_anova.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNdbJdkLOfEF",
        "outputId": "9f5e382b-54f7-4da3-b288-603f24a2aaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column: 0 - ANOVA p-value: 0.0\n",
            "Column: 1 - ANOVA p-value: 0.0\n",
            "Column: 2 - ANOVA p-value: 0.0\n",
            "Column: 3 - ANOVA p-value: 0.0\n",
            "Column: 4 - ANOVA p-value: 0.0003805229753154665\n",
            "Column: 6 - ANOVA p-value: 0.0\n",
            "Column: 12 - ANOVA p-value: 0.0\n",
            "Column: 14 - ANOVA p-value: 0.0\n",
            "Column: 15 - ANOVA p-value: 0.0\n",
            "Column: 16 - ANOVA p-value: 0.0\n",
            "Column: 18 - ANOVA p-value: 0.0\n",
            "Column: 19 - ANOVA p-value: 0.0\n",
            "Column: 21 - ANOVA p-value: 0.0\n",
            "Column: 22 - ANOVA p-value: 0.0\n",
            "Column: 23 - ANOVA p-value: 0.0\n",
            "Column: 24 - ANOVA p-value: 5.021119420792319e-05\n",
            "Column: 25 - ANOVA p-value: 0.0\n",
            "Column: 28 - ANOVA p-value: 0.0\n",
            "Column: 29 - ANOVA p-value: 0.0\n",
            "Column: 31 - ANOVA p-value: 0.0\n",
            "Column: 32 - ANOVA p-value: 0.0\n",
            "Column: 33 - ANOVA p-value: 0.0\n",
            "Column: 34 - ANOVA p-value: 0.0\n",
            "Column: 35 - ANOVA p-value: 0.0\n",
            "Column: 37 - ANOVA p-value: 0.0\n",
            "Column: 38 - ANOVA p-value: 0.0\n",
            "Column: 39 - ANOVA p-value: 0.0\n",
            "Column: 40 - ANOVA p-value: 0.0\n",
            "Column: 41 - ANOVA p-value: 0.0\n",
            "Column: 43 - ANOVA p-value: 0.0\n",
            "Column: 44 - ANOVA p-value: 0.0\n",
            "Column: 45 - ANOVA p-value: 0.0\n",
            "Column: 47 - ANOVA p-value: 0.0\n",
            "Column: 48 - ANOVA p-value: 0.0\n",
            "Column: 51 - ANOVA p-value: 0.0\n",
            "Column: 55 - ANOVA p-value: 0.0\n",
            "Column: 59 - ANOVA p-value: 1.2552004335164544e-54\n",
            "Column: 61 - ANOVA p-value: 0.0\n",
            "Column: 62 - ANOVA p-value: 0.0\n",
            "Column: 63 - ANOVA p-value: 0.0\n",
            "Column: 64 - ANOVA p-value: 0.0\n",
            "Column: 67 - ANOVA p-value: 0.023676043778288868\n",
            "Column: 68 - ANOVA p-value: 0.0\n",
            "Column: 71 - ANOVA p-value: 0.0\n",
            "Column: 72 - ANOVA p-value: 0.0\n",
            "Column: 74 - ANOVA p-value: 0.0\n",
            "Column: 83 - ANOVA p-value: 0.0\n",
            "Column: 88 - ANOVA p-value: 0.0\n",
            "Column: 90 - ANOVA p-value: 0.0\n",
            "Column: 96 - ANOVA p-value: 0.0\n",
            "Column: 110 - ANOVA p-value: 0.0\n",
            "Column: 111 - ANOVA p-value: 0.0\n",
            "Column: 115 - ANOVA p-value: 0.0\n",
            "Column: 117 - ANOVA p-value: 0.0\n",
            "Column: 120 - ANOVA p-value: 0.0\n",
            "Column: 122 - ANOVA p-value: 0.0\n",
            "Column: 123 - ANOVA p-value: 0.0\n",
            "Column: 125 - ANOVA p-value: 0.0\n",
            "Column: 126 - ANOVA p-value: 0.0\n",
            "Column: 128 - ANOVA p-value: 0.0\n",
            "Column: 129 - ANOVA p-value: 1.0535630316652931e-20\n",
            "Column: 133 - ANOVA p-value: 0.0\n",
            "Column: 134 - ANOVA p-value: 0.0\n",
            "Column: 135 - ANOVA p-value: 0.0\n",
            "Column: 136 - ANOVA p-value: 0.0\n",
            "Column: 137 - ANOVA p-value: 0.0\n",
            "Column: 138 - ANOVA p-value: 0.0\n",
            "Column: 139 - ANOVA p-value: 0.0\n",
            "Column: 142 - ANOVA p-value: 0.0\n",
            "Column: 150 - ANOVA p-value: 0.0\n",
            "Column: 151 - ANOVA p-value: 1.4166449051091726e-76\n",
            "Column: 158 - ANOVA p-value: 0.0\n",
            "Column: 159 - ANOVA p-value: 3.873435589966139e-233\n",
            "Column: 160 - ANOVA p-value: 1.3135685778323096e-263\n",
            "Column: 161 - ANOVA p-value: 5.651686388148162e-260\n",
            "Column: 162 - ANOVA p-value: 8.401617720519068e-164\n",
            "Column: 163 - ANOVA p-value: 0.0\n",
            "Column: 166 - ANOVA p-value: 0.0\n",
            "Column: 167 - ANOVA p-value: 0.0\n",
            "Column: 169 - ANOVA p-value: 0.0\n",
            "Column: 170 - ANOVA p-value: 0.0\n",
            "Column: 175 - ANOVA p-value: 0.0\n",
            "Column: 177 - ANOVA p-value: 0.0\n",
            "Column: 180 - ANOVA p-value: 0.0\n",
            "Column: 181 - ANOVA p-value: 0.0\n",
            "Column: 182 - ANOVA p-value: 0.0\n",
            "Column: 183 - ANOVA p-value: 0.0\n",
            "Column: 184 - ANOVA p-value: 0.0\n",
            "Column: 185 - ANOVA p-value: 0.0\n",
            "Column: 188 - ANOVA p-value: 0.0\n",
            "Column: 195 - ANOVA p-value: 0.0\n",
            "Column: 198 - ANOVA p-value: 0.0\n",
            "Column: 200 - ANOVA p-value: 0.0\n",
            "Column: 201 - ANOVA p-value: 0.0\n",
            "Column: 208 - ANOVA p-value: 0.0\n",
            "Column: 218 - ANOVA p-value: 0.0\n",
            "Column: 223 - ANOVA p-value: 0.0\n",
            "Column: 245 - ANOVA p-value: 5.80826161681738e-107\n",
            "Column: 255 - ANOVA p-value: 0.0\n",
            "Column: 268 - ANOVA p-value: 0.0\n",
            "Column: 269 - ANOVA p-value: 0.0\n",
            "Column: 416 - ANOVA p-value: 0.0\n",
            "Column: 417 - ANOVA p-value: 0.0\n",
            "Column: 418 - ANOVA p-value: 0.0\n",
            "Column: 419 - ANOVA p-value: 9.552745614823107e-257\n",
            "Column: 423 - ANOVA p-value: 0.0\n",
            "Column: 426 - ANOVA p-value: 0.0\n",
            "Column: 429 - ANOVA p-value: 2.8546782689906276e-183\n",
            "Column: 432 - ANOVA p-value: 7.812152995013836e-189\n",
            "Column: 433 - ANOVA p-value: 7.717688534101183e-240\n",
            "Column: 438 - ANOVA p-value: 0.0\n",
            "Column: 439 - ANOVA p-value: 0.0\n",
            "Column: 442 - ANOVA p-value: 0.0\n",
            "Column: 443 - ANOVA p-value: 0.0\n",
            "Column: 444 - ANOVA p-value: 0.0\n",
            "Column: 460 - ANOVA p-value: 0.0\n",
            "Column: 468 - ANOVA p-value: 6.409943849569832e-267\n",
            "Column: 472 - ANOVA p-value: 0.0\n",
            "Column: 474 - ANOVA p-value: 0.0\n",
            "Column: 476 - ANOVA p-value: 0.0\n",
            "Column: 482 - ANOVA p-value: 0.0\n",
            "Column: 483 - ANOVA p-value: 1.92856351133e-312\n",
            "Column: 484 - ANOVA p-value: 2.61571078101574e-284\n",
            "Column: 485 - ANOVA p-value: 9.761371762975727e-244\n",
            "Column: 486 - ANOVA p-value: 2.6860343455175406e-303\n",
            "Column: 487 - ANOVA p-value: 3.675565124765342e-238\n",
            "Column: 488 - ANOVA p-value: 0.0\n",
            "Column: 489 - ANOVA p-value: 0.0\n",
            "Column: 491 - ANOVA p-value: 0.0\n",
            "Column: 492 - ANOVA p-value: 0.0\n",
            "Column: 493 - ANOVA p-value: 0.0\n",
            "Column: 494 - ANOVA p-value: 4.251939667888049e-27\n",
            "Column: 496 - ANOVA p-value: 0.0\n",
            "Column: 499 - ANOVA p-value: 1.3626860172168723e-196\n",
            "Column: 500 - ANOVA p-value: 2.2063853113344103e-170\n",
            "Column: 510 - ANOVA p-value: 0.0\n",
            "Column: 511 - ANOVA p-value: 8.521260175759374e-208\n",
            "Column: 519 - ANOVA p-value: 7.913113072865541e-183\n",
            "Column: 520 - ANOVA p-value: 1.0658612203668937e-122\n",
            "Column: 521 - ANOVA p-value: 1.7490564973490379e-06\n",
            "Column: 523 - ANOVA p-value: 3.9933876625832445e-35\n",
            "Column: 525 - ANOVA p-value: 0.0\n",
            "Column: 526 - ANOVA p-value: 0.0\n",
            "Column: 539 - ANOVA p-value: 0.0\n",
            "Column: 546 - ANOVA p-value: 0.0\n",
            "Column: 547 - ANOVA p-value: 0.0\n",
            "Column: 548 - ANOVA p-value: 0.0\n",
            "Column: 549 - ANOVA p-value: 0.0\n",
            "Column: 550 - ANOVA p-value: 0.0\n",
            "Column: 551 - ANOVA p-value: 0.0\n",
            "Column: 559 - ANOVA p-value: 0.0\n",
            "Column: 562 - ANOVA p-value: 0.0\n",
            "Column: 563 - ANOVA p-value: 0.0\n",
            "Column: 564 - ANOVA p-value: 0.0\n",
            "Column: 569 - ANOVA p-value: 0.0\n",
            "Column: 570 - ANOVA p-value: 0.0\n",
            "Column: 571 - ANOVA p-value: 0.0\n",
            "Column: 572 - ANOVA p-value: 4.082424901870454e-40\n",
            "Column: 573 - ANOVA p-value: 0.0\n",
            "Column: 581 - ANOVA p-value: 2.0775704433733e-310\n",
            "Column: 585 - ANOVA p-value: 2.7656e-319\n",
            "Number of Columns after Correlation-  101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJH_VA1QV7sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chi-Squared\n",
        "# -> Our Data is Numerical so chi-squared can't be done."
      ],
      "metadata": {
        "id": "2qPk5yg8MNMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of Data after Feature Selection\n",
        "\n",
        "print(\"Shape - \", data_anova.shape)\n",
        "\n",
        "# Filling NaN Values with random \n",
        "for column in data_anova.columns:\n",
        "    # print(\"Column- \", column)\n",
        "    # Get the minimum and maximum values of the column\n",
        "    min_value = data_anova[column].min()\n",
        "    max_value = data_anova[column].max()\n",
        "\n",
        "    # Generate random numbers within the range\n",
        "    random_values = np.random.uniform(min_value, max_value, size=data_anova[column].isnull().sum())\n",
        "    \n",
        "    # Create a Series with the random values\n",
        "    random_series = pd.Series(random_values, index=data_anova[column][data_anova[column].isnull()].index)\n",
        "\n",
        "    # Fill NaN values with the random series\n",
        "    data_anova[column].fillna(random_series, inplace=True)\n",
        "\n",
        "    # Print\n",
        "    # print(data[column].isnull().sum())\n",
        "\n",
        "\n",
        "# Separate features and target\n",
        "X = data_anova.drop('Pass/Fail', axis=1)\n",
        "y = data_anova['Pass/Fail']\n",
        "\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "\n",
        "# Initialize and train logistic regression model\n",
        "log_reg = LogisticRegression(max_iter=10000)  # Increase max_iter if it doesn't converge\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghXxuFQrZusb",
        "outputId": "c28e4a05-3478-4369-e1c5-c24481eff862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape -  (1567, 101)\n",
            "(1253, 100)\n",
            "(314, 100)\n",
            "Test accuracy: 0.9076433121019108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqPvVndUeMz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank You"
      ],
      "metadata": {
        "id": "p42vPltWfSuQ"
      }
    }
  ]
}